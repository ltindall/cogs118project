{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Authors: Lucas Tindall and Andrew Saad\n",
    "\n",
    "# Import numpy and random for working with matrices. \n",
    "import numpy as np\n",
    "import random \n",
    "\n",
    "# Import mnist_loader to load the MNIST dataset in numpy format.\n",
    "# mnist_loader provided from https://github.com/mnielsen/neural-networks-and-deep-learning\n",
    "import mnist_loader\n",
    "\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NetworkLayer\n",
    "#\n",
    "# The hidden layer and the output layer are groups of neurons. \n",
    "#\n",
    "class NetworkLayer: \n",
    "    \n",
    "    # Initialize the network layer. \n",
    "    #\n",
    "    # Paramters: \n",
    "    #  - weights: a numpy matrix of initial weights (dimensions are [# of neurons x # of inputs]) \n",
    "    #      rows of weight matrix correspond to each neurons weight\n",
    "    #      (row i contains the weights leading into the i'th neuron of this layer)\n",
    "    #  - biases: a numpy matrix (dimensions are [# of neurons x 1]) of initial biases\n",
    "    def __init__(self, weights, biases): \n",
    "        \n",
    "        # self.weights\n",
    "        #  Each network layer has a matrix of weights. \n",
    "        #   - The matrix of weights has a format where the rows correspond to \n",
    "        #     each hidden/output layer neuron and the columns refer to the inputs \n",
    "        #     to that layer. For example the weight in the first row, second column \n",
    "        #     of the hidden layer weights corresponds to the weight from the second\n",
    "        #     input to the first hidden neuron. \n",
    "        self.weights = weights\n",
    "        \n",
    "        # self.biases\n",
    "        #  Each network layer also has a vertex of biases. \n",
    "        #   - There is one bias value for each neuron.  \n",
    "        self.biases = biases \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NeuralNetwork\n",
    "#\n",
    "# This NeuralNetwork class represents a 3 layer neural network. \n",
    "# Groups of neurons form layers in the neural network. \n",
    "# The first layer is just the inputs (no need to represent these as neurons). \n",
    "# The second layer is the hidden layer. \n",
    "# The third layer is the output layer. \n",
    "# \n",
    "class NeuralNetwork: \n",
    "    \n",
    "    # Initialize the network with sizes for each layer. \n",
    "    #\n",
    "    # Parameters: \n",
    "    #  - input_layer_size: number of inputs \n",
    "    #  - hidden_layer_size: number of neurons in hidden layer \n",
    "    #  - output_layer_size: number of outputs in hidden layer \n",
    "    def __init__(self, input_layer_size, hidden_layer_size, output_layer_size):\n",
    "        \n",
    "        ### Input layer is just training data. For the MNIST data each input represents \n",
    "        ### a pixel in the input images. There are 784 inputs representing a 28*28 pixel image. \n",
    "        \n",
    "        \n",
    "        ### Start Hidden layer\n",
    "        # Each neuron in the hidden layer has a weight connecting it to every point in the training data.\n",
    "        # The hidden layer can have a variable number of neurons. Adding more neurons generally helps with \n",
    "        # training, at least up to a certain point, at which the network starts to overfit. \n",
    "\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        \n",
    "        # assign random initial weights \n",
    "        rand_hidden_weights = np.random.randn(hidden_layer_size, input_layer_size)\n",
    "        \n",
    "        # assign random initial biases \n",
    "        rand_hidden_biases = np.random.randn(hidden_layer_size, 1)\n",
    "\n",
    "        # initialize the hidden layer \n",
    "        self.hidden_layer = NetworkLayer(rand_hidden_weights, rand_hidden_biases)\n",
    "       \n",
    "        ### End Hidden Layer \n",
    "        \n",
    "        \n",
    "        ### Start Output Layer\n",
    "        # Each output neuron has a weight connecting it every output of the hidden layer. \n",
    "        # Our output layer has 10 neurons to represent one-hot encoding of the handwritten digits. \n",
    "        \n",
    "        self.output_layer_size = output_layer_size \n",
    "        \n",
    "        # assign random initial weights \n",
    "        rand_output_weights = np.random.randn(output_layer_size, hidden_layer_size)\n",
    "        \n",
    "        # assign random initial biases \n",
    "        rand_output_biases = np.random.randn(output_layer_size, 1)\n",
    "        \n",
    "        # initialize the output layer \n",
    "        self.output_layer = NetworkLayer(rand_output_weights, rand_output_biases)\n",
    "        \n",
    "        ### End Output Layer \n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "            \n",
    "    # Train the network using training data. \n",
    "    # \n",
    "    # Parameters: \n",
    "    #  - training_data: numpy array of training data points \n",
    "    #  - test_data: numpy array of test data points \n",
    "    #  - epochs: number of times to run the training loop \n",
    "    #  - batch_size: batch size to use for training\n",
    "    #  - alpha: learning rate \n",
    "    def train(self, training_data, validation_data, test_data, epochs, batch_size, alpha): \n",
    "        \n",
    "        old_percent_correct = 0\n",
    "        \n",
    "        # Loop over all epochs \n",
    "        for i in range(epochs): \n",
    "            \n",
    "            ################################\n",
    "            # Start BATCH CODE\n",
    "            # We ended up not using batch code since our initial results \n",
    "            # performed well. \n",
    "            ################################\n",
    "            # shuffle the training data \n",
    "            #random.shuffle(training_data)\n",
    "            \n",
    "            # create a set of batches according to batch_size\n",
    "            #batch_groups = []\n",
    "            #for i in xrange(0, len(training_data), batch_size): \n",
    "            #    batch_groups.append(training_data[i:i+batch_size])\n",
    "            \n",
    "            # for each batch \n",
    "            #for batch in batch_groups: \n",
    "                # run forward_pass\n",
    "                \n",
    "                # run backpropagation (and update weights)\n",
    "            ###################################\n",
    "            # End BATCH CODE\n",
    "            ###################################\n",
    "            \n",
    "            \n",
    "            # Perform forward pass and backpropagation \n",
    "            for inputs,outputs in training_data: \n",
    "                self.forward_pass(inputs)\n",
    "                # weights are updated in backpropagation method \n",
    "                self.backpropagation(inputs, outputs, alpha)\n",
    "            \n",
    "            \n",
    "            correct_validation = self.run_test(validation_data, False)\n",
    "            percent_correct_validation = correct_validation/float(len(validation_data)) * 100\n",
    "            # Test new weights on test data and print results \n",
    "            print \"Epoch \"+str(i)+\" validation results: \",\n",
    "            print (str(correct_validation) + \"/\" + str(len(validation_data)) \n",
    "                  + \" = \" + str(percent_correct_validation) +\"%\") \n",
    "            if percent_correct_validation > 96: \n",
    "                break\n",
    "            if percent_correct_validation > 95.5: \n",
    "                if old_percent_correct > percent_correct_validation: \n",
    "                    break\n",
    "            old_percent_correct = percent_correct_validation\n",
    "                \n",
    "        correct_test = self.run_test(test_data, True)\n",
    "        percent_correct_test = correct_test/float(len(test_data)) * 100\n",
    "        print \"Final Run on test data: \",\n",
    "        print (str(correct_test) + \"/\" + str(len(test_data)) \n",
    "                  + \" = \" + str(percent_correct_test) +\"%\")\n",
    "\n",
    "            \n",
    "    # Evaluate the accuracy of the weights on the test data set. \n",
    "    #\n",
    "    # Parameters: \n",
    "    #  - test_data: The test data set\n",
    "    def run_test(self, test_data, print_plots): \n",
    "        \n",
    "        network_test_predictions = []\n",
    "        expected_test_output = []\n",
    "        \n",
    "        # run forward pass on the test data set and retrieve the predicted output   \n",
    "        for inputs, outputs in test_data: \n",
    "            network_test_predictions.append(self.forward_pass(inputs))\n",
    "            expected_test_output.append(outputs)\n",
    "        \n",
    "    \n",
    "        # conver the one-hot encoded output into the correct digit value \n",
    "        network_test_output = np.argmax(np.asarray(network_test_predictions), axis=1)\n",
    "        \n",
    "        \n",
    "        correct = 0\n",
    "        j = 1\n",
    "        if (print_plots): \n",
    "            print \"\"\n",
    "            print \"Sample of incorrect predictions from test data set\"\n",
    "            print \"Ex = Expected, Pr = Predicted\"\n",
    "            print \"\"\n",
    "            fig = pyplot.figure()\n",
    "            fig.subplots_adjust(hspace=1)\n",
    "        # loop over all the outputs and compare the prediction against the actual value \n",
    "        for i in range(len(expected_test_output)): \n",
    "            correct = correct + int(network_test_output[i][0] == expected_test_output[i])\n",
    "            \n",
    "            if (print_plots and j < 26):\n",
    "                if(int(network_test_output[i][0] != expected_test_output[i])):\n",
    "                    reshaped_image = test_data[i][0].reshape((28,28))\n",
    "                    ax = fig.add_subplot(5,5,j)\n",
    "                    ax.set_title('Ex: '+str(expected_test_output[i])+',Pr:'+str(network_test_output[i][0]))\n",
    "                    imgplot = ax.imshow(reshaped_image, cmap='gray')\n",
    "                    pyplot.xticks(np.array([]))\n",
    "                    pyplot.yticks(np.array([]))\n",
    "                    j = j+1\n",
    "        if (print_plots): \n",
    "            pyplot.show()\n",
    "            print \"\"\n",
    "\n",
    "        return correct\n",
    "        \n",
    "    \n",
    "    # Perform backprogation through the neural network layers and update the weights/biases matrices. \n",
    "    #\n",
    "    # Paramters: \n",
    "    #  - inputs: current inputs for this training loop (one handwritten digit image)\n",
    "    #  - outputs the expected outputs for this training loop (the actual digit)\n",
    "    #  - alpha: learning rate \n",
    "    def backpropagation(self, inputs, outputs, alpha): \n",
    "        \n",
    "        # Calculate delta of all neurons \n",
    "        # Delta is different for hidden layer neurons vs. output layer neurons \n",
    "        \n",
    "        # Delta for output layer neurons \n",
    "        #\n",
    "        # delta for output layer neurons = \n",
    "        #   (output of current output neuron - target) \n",
    "        #   * (output of current output neuron)  \n",
    "        #   * (1 - output of current output neuron) \n",
    "        #\n",
    "        # size of output deltas = size of output neuron layer (one delta for each neuron) \n",
    "\n",
    "        self.output_deltas = ((self.output_layer_output - outputs) \n",
    "                              * (self.output_layer_output * (1 - self.output_layer_output)))\n",
    "        \n",
    "        # Delta for hidden layer neurons \n",
    "        # \n",
    "        # delta for hidden layer neurons = \n",
    "        #   [for each output layer neuron(\n",
    "        #     (output of output neuron - target)  \n",
    "        #     * (output of output neuron) * (1 - output of output neuron) \n",
    "        #     * (output layer weight from the current hidden neuron to for loop output neuron)  )]\n",
    "        #   * (output of current hidden neuron) * (1 - output of current hidden neuron)\n",
    "        #\n",
    "        # size of hidden deltas = size of hidden neuron layer \n",
    "        \n",
    "        # calculate the summation portion of the delta \n",
    "        temp_sum = np.dot(self.output_layer.weights.transpose(), self.output_deltas)\n",
    "\n",
    "        # calculate the hidden layer deltas \n",
    "        self.hidden_deltas = (temp_sum * self.hidden_layer_output * (1 - self.hidden_layer_output))\n",
    "        \n",
    "        # gradient = delta of neuron * input to neuron \n",
    "        # gradients for hidden to output weights = np.dot( delta of output neurons, outputs of hidden neurons )\n",
    "        # gradients for input to hidden weights = np.dot( delta of hidden neurons, inputs to network )\n",
    "        # gradient matrix should match size of weights matrix \n",
    "        self.output_gradients = np.dot(self.output_deltas, self.hidden_layer_output.transpose())\n",
    "        self.hidden_gradients = np.dot(self.hidden_deltas, self.network_input.transpose())\n",
    "        \n",
    "        # bias gradients are equal to the deltas \n",
    "        self.output_bias_gradients = self.output_deltas\n",
    "        self.hidden_bias_gradients = self.hidden_deltas\n",
    "        \n",
    " \n",
    "        # update amount = learning rate * -1 * gradient\n",
    "        output_update_amounts = alpha * -1 * self.output_gradients \n",
    "        hidden_update_amounts = alpha * -1 * self.hidden_gradients \n",
    "        \n",
    "        output_bias_update_amounts = alpha * -1 * self.output_bias_gradients\n",
    "        hidden_bias_update_amounts = alpha * -1 * self.hidden_bias_gradients \n",
    "        \n",
    "        \n",
    "        # update the weights by the update amounts \n",
    "        self.output_layer.weights = self.output_layer.weights + output_update_amounts\n",
    "        self.hidden_layer.weights = self.hidden_layer.weights + hidden_update_amounts \n",
    "        \n",
    "        self.output_layer.biases = self.output_layer.biases + output_bias_update_amounts \n",
    "        self.hidden_layer.biases = self.hidden_layer.biases + hidden_bias_update_amounts \n",
    "        \n",
    "    # Perform forward pass through the network with the input data \n",
    "    #\n",
    "    # Parameters: \n",
    "    #  - data: input data for this training loop \n",
    "    def forward_pass(self, data): \n",
    "\n",
    "        \n",
    "        ### Forward pass through hidden layer \n",
    "\n",
    "        # input for hidden layer \n",
    "        self.network_input = data\n",
    "        \n",
    "        # dot product of hidden layer weights and network inputs \n",
    "        hidden_layer_net_input = np.dot(self.hidden_layer.weights, self.network_input) + self.hidden_layer.biases\n",
    "        \n",
    "        # apply activation(sigmoid) to dot product \n",
    "        self.hidden_layer_output = self.sigmoid(hidden_layer_net_input)\n",
    "        \n",
    "        \n",
    "        ### Forward pass through output layer \n",
    "        \n",
    "        # input for output layer is output of hidden layer \n",
    "        output_layer_input = self.hidden_layer_output\n",
    "        \n",
    "        # dot product of output layer weights and hidden layer output \n",
    "        output_layer_net_input = np.dot(self.output_layer.weights, output_layer_input) + self.output_layer.biases \n",
    "        \n",
    "        # apply activation(sigmoid) to dot product \n",
    "        self.output_layer_output = self.sigmoid(output_layer_net_input)\n",
    "        \n",
    "        # return output layer output (one-hot encoded prediction of the handwritten digit)\n",
    "        return self.output_layer_output\n",
    "        \n",
    "    # sigmoid function \n",
    "    #\n",
    "    # Parameters: \n",
    "    #  - t: input to sigmoid function \n",
    "    def sigmoid(self,t):\n",
    "        return 1.0/(1.0+np.exp(-t))\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 validation results:  9127/10000 = 91.27%\n",
      "Epoch 1 validation results:  9348/10000 = 93.48%\n",
      "Epoch 2 validation results:  9355/10000 = 93.55%\n",
      "Epoch 3 validation results:  9387/10000 = 93.87%\n",
      "Epoch 4 validation results:  9390/10000 = 93.9%\n",
      "Epoch 5 validation results:  9466/10000 = 94.66%\n",
      "Epoch 6 validation results:  9467/10000 = 94.67%\n"
     ]
    }
   ],
   "source": [
    "# mnist_loader provided from https://github.com/mnielsen/neural-networks-and-deep-learning\n",
    "# use mnist_loader function to load MNIST data as numpy formatted arrays \n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "\n",
    "\n",
    "# initialize the Neural Network with 40 hidden neurons (2nd parameter)\n",
    "neuralnet = NeuralNetwork(len(training_data[0][0]), 40, 10)\n",
    "\n",
    "# train the neural net over 1000 epochs (3rd parameter) with learning rate 0.7 (5th parameter)\n",
    "neuralnet.train(training_data, validation_data, test_data, 1000, 777, .7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
